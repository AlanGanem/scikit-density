# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/02_core.random_variable.ipynb (unless otherwise specified).

__all__ = ['identity_func', 'agg_smallest_distance', 'KDE', 'IDENTITY_TRANSFORMER', 'Empirical', 'RandomVariable',
           'CustomArray', 'RVArray']

# Cell
from functools import partial
from warnings import warn

import scipy
import scipy.stats as stats
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import QuantileTransformer, FunctionTransformer
from sklearn.neighbors import KernelDensity
from sklearn.decomposition import PCA, KernelPCA

import KDEpy as kdepy
import awkde

from ..utils import (
    cos_sim_query, sample_multi_dim, ctqdm, DelegateEstimatorMixIn, _vector_1d_to_matrix,_assert_dim_3d,_assert_dim_2d,
    add_noise, _fix_X_1d, draw_from, _fix_one_sample_2d, _fix_one_dist_2d, _fix_dist_1d
)

# Cell

#Identity transformer in case space_transformer is None
def identity_func(x):
    return x

IDENTITY_TRANSFORMER = FunctionTransformer(
    func = identity_func,
    inverse_func = identity_func,
    validate=False,
    accept_sparse=True,
    check_inverse=True,
    kw_args=None,
    inv_kw_args=None,
)

def agg_smallest_distance(data, agg_func = np.mean):
    '''
    returns the agregate (defined by agg_func) distance of each point and their closest neighbor
    recieves array of shape (n_dists,n_samples, n_dims) and reutrns array of shape (n_dists, n_dims)
    '''
    _assert_dim_3d(data)
    data = np.sort(data, axis = 1)
    diff = np.diff(data, axis = 1)
    results = agg_func(diff, axis = 1)
    return results

class KDE():

    AVALIBLE_BW_METHODS = ['ISJ', 'scott', 'silverman', 'mean_distance', 'std_distance', 'median_distance']

    def __init__(self, bw = 'std_distance', space_transformer = PCA, implementation = 'sklearn', st_kws = {}, **kde_kws):

        if bw.__class__ == str:
            assert bw in self.AVALIBLE_BW_METHODS, f"if str, bw should be one of {self.AVALIBLE_BW_METHODS}, not {bw}"
        if not isinstance(bw,(str, float, np.float64, np.float32, np.float)):
            raise TypeError(f'bw should be str or float, not {bw.__class__}')
        self.bw = bw
        self._space_transformer = space_transformer if not space_transformer is None else IDENTITY_TRANSFORMER
        self.kde_kws = kde_kws
        self.st_kws = st_kws
        if not implementation in ['scipy','sklearn','awkde']:
            raise ValueError(f'implementation should be one of ["sklearn","scipy","awkde"], not {implementation}')

        self.implementation = implementation

    def _check_X_2d(self,X):
        X = np.array(X)
        #reshape if shape == (n_samples,)
        X = X if len(X.shape) > 1 else X.reshape(-1,1)
        return X

    def _check_input_dims_match(self, X):
        if X.shape[-1] != self.n_dim:
            raise ValueError(f'X dimensions space should be the same size as fitted distribution ({self.n_dim}), got {X.shape[-1]} instead')

    def _get_bw_each_dim(self, X, bw_method):
        if bw_method in ['ISJ', 'scott', 'silverman']:
            return np.array([kdepy.FFTKDE(bw = bw_method).bw(X[:,i:i+1]) for i in range(X.shape[-1])])
        elif bw_method == 'mean_distance':
            return np.array([agg_smallest_distance(X[:,i].reshape(1,X.shape[0],1), np.mean) for i in range(X.shape[-1])])
        elif bw_method == 'median_distance':
            return np.array([agg_smallest_distance(X[:,i].reshape(1,X.shape[0],1), np.median) for i in range(X.shape[-1])])
        elif bw_method == 'std_distance':
            return np.array([agg_smallest_distance(X[:,i].reshape(1,X.shape[0],1), np.std) for i in range(X.shape[-1])])

    def _preprocess_fit(self, X):
        '''
        preprocess data prior to fit. ensure len >2 and add some white noise to avoid eigenvalues errors in space transform
        '''
        X = self._check_X_2d(X)
        if len(X) < 2:
            X = np.concatenate([X,X])
        X = add_noise(X, 1e-9)
        return X

    def fit(self, X, y = None, sample_weight = None):
        #preprocess X
        X = self._preprocess_fit(X)
        #fit and transform X with manifold learner (self.space_transformer)
        if isinstance(self._space_transformer, type):
            self._space_transformer = self._space_transformer(**{**self.st_kws, **{
                'n_components':X.shape[-1], 'whiten':True}})

        X = self._space_transformer.fit_transform(X)
        # calculate bw
        if self.bw.__class__ == str:
            bw = self._get_bw_each_dim(X, self.bw)
            bw = np.sqrt(np.sum(bw**2))
        else:
            warn('passing a float value for bw is not recomended since X will be transformed by space_transformer before fitting and bw value may not make sence in new trnasformed space')
            bw = self.bw

        #ensure bw is positive
        bw = max(1e-4, bw)
        #kde
        if self.implementation == 'sklearn':
            self.estimator = KernelDensity(**{**{'bandwidth':bw},**self.kde_kws}).fit(X, y, sample_weight = sample_weight)
        elif self.implementation == 'scipy':
            self.estimator = stats.gaussian_kde(X.T, bw_method = bw)
        elif self.implementation == 'awkde':
            self.estimator = awkde.GaussianKDE(**{**{'glob_bw':bw},**self.kde_kws})
            self.estimator.fit(X = X, weights = sample_weight)
        else: raise ValueError(f'self.implementation should be one of ["sklearn","scipy","awkde"], not {self.implementation}')

        self._transformed_bw_value = bw
        self.n_dim = X.shape[-1]
        return self

    def evaluate(self, data):
        data = self._check_X_2d(data)
        #transform input
        data = self._space_transformer.transform(data)
        self._check_input_dims_match(data)
        #get likelihoods
        if self.implementation == 'sklearn':
            likelihood = np.exp(self.estimator.score_samples(data))
        elif self.implementation == 'scipy':
            likelihood = self.estimator.pdf(data.T)
        elif self.implementation == 'awkde':
            likelihood = self.estimator.predict(data)
        else: raise ValueError(f'self.implementation should be one of ["sklearn","scipy","awkde"], not {self.implementation}')

        return likelihood

    def predict(self, X):
        return self.evaluate(X)

    def pdf(self, data):
        return self.evaluate(data)

    def rvs(self, size = 1, random_state = None):
        sample_size = size
        if self.implementation == 'sklearn':
            samples = self.estimator.sample(n_samples = sample_size, random_state = random_state)
        elif self.implementation == 'scipy':
            samples = self.estimator.resample(sample_size, random_state).T
        elif self.implementation == 'awkde':
            samples = self.estimator.sample(n_samples = sample_size, random_state = random_state)
        else: raise ValueError(f'self.implementation should be one of ["sklearn","scipy","awkde"], not {self.implementation}')
        #inverse transform samples
        samples = self._space_transformer.inverse_transform(samples)
        return samples

    def sample(self, sample_size = 1, random_state = None):
        return self.rvs(sample_size, random_state)

    def entropy(self, sample_size = 100):
        return np.mean(-np.log2(self.evaluate(self.rvs(size = sample_size))))

    def cdf(self, data, sample_size = 1000):
        samples = self.sample(sample_size = sample_size)
        # fix shape in order to work with _quantile
        samples = samples.reshape(1, *samples.shape)
        return _quantile(data.reshape(1, *data.shape), samples)

    def ppf(self, data, sample_size = 100):
        #estimate using sampling and QuantileTransformer since integration is too costly
        data = np.array(data)
        assert (data.min() >= 0) and (data.max() <= 1), 'data contains values < 0 or > 1'
        samples = self.sample(sample_size = sample_size)
        return QuantileTransformer(n_quantiles = min(1000,samples.shape[0])).fit(samples).inverse_transform(data)

    def _make_conditioning_grid(self, condition_dict = {}, resolution = None):
        samples, likelihood = self.sample(1000) #estimate min and max intervals
        argsrt = np.argsort(likelihood)[::-1]
        likelihood_msk = likelihood[argsrt].cumsum() < 0.99*likelihood.sum()
        likelihood_msk = argsrt[likelihood_msk]
        #ignore points with low likelihood
        grid_min, grid_max = samples[likelihood_msk].min(axis = 0), samples[likelihood_msk].max(axis = 0)
        dim_grid = []
        for dim in range(grid_min.shape[0]):
            dim_min, dim_max = grid_min[dim], grid_max[dim]
            if not dim in condition_dict:
                dim_grid.append(np.linspace(dim_min,dim_max, resolution))
            else:
                dim_grid.append(np.linspace(condition_dict[dim],condition_dict[dim], resolution))
        return np.array(dim_grid).T


# Cell
def _check_kde_metrics_input(y_true, y_dists, frac):
    '''
    preprocesses inputs for kde metrics calculation
    '''
    y_dists = _assert_dim_3d(y_dists)

    if len(y_true.shape) <= 3:
        assert y_true.shape[0] == y_dists.shape[0], f'y_true dim 0 should be equal y_dists dim 0, got {y_true.shape[0]} and {y_dists.shape[0]}'
    else:
        raise Exception(f'y_true dims should less or equal to 3, got {len(y_true.shape)}')

    y_true = _fix_one_sample_2d(y_true)

    idxs = np.arange(y_true.shape[0])
    idxs = draw_from(idxs, frac)

    y_dists = y_dists[idxs]
    y_true = y_true[idxs]
    return y_true, y_dists


# Cell
def _kde_entropy(data, sample_size = 200, frac = 1.0, progress_bar = False, **kde_kwargs):
    '''
    Calculates the entropy of multiple continuous distributions. entropy equals np.mean(-np.log(p(x)))
    input should be of shape (n_distributions, n_sample_per_distribution, n_dims_in_distribtuion)
    '''
    data = _fix_one_dist_2d(data)
    data = _assert_dim_3d(data)
    kde = KDE(**kde_kwargs)
    data = draw_from(data, frac)
    if progress_bar:
        return np.array([kde.fit(d).entropy(sample_size = sample_size) for d in tqdm(data)])
    else:
        return np.array([kde.fit(d).entropy(sample_size = sample_size) for d in data])

# Cell
def _ppf(percentiles, y_dists):
    '''
    returns the percent point function for given y_dists and percentiles
    expected dims:
    percentiles: (n_dists, n_points)
    y_dists: (n_dists, n_samples, dims)
    '''
    assert len(percentiles.shape) == 2, f'percentiles should have 2 dimensions: (n_dists, n_percentiles), got {len(percentiles.shape)}'
    assert len(y_dists.shape) == 3, f'y_dists should have 3 dimensions: (n_dists, n_samples, n_dims), got {len(y_dists.shape)}'
    assert percentiles.shape[0] == y_dists.shape[0], f'percentiles n_dists should be equal y_dists n_dists. got {percentiles.shape[0]} and {y_dists.shape[0]}'
    values = np.array([np.quantile(y_dists[i], percentiles[i], axis = 0) for i in range(y_dists.shape[0])])
    return _fix_one_sample_2d(values)

# Cell
def _kde_likelihood(y_true,y_dists, frac = 1.0, progress_bar = False,**kde_kwargs):
    '''
    Calculates the likelihood of y_true in kde estimation of samples
    input should be of shape (n_distributions, n_sample_per_distribution, n_dims_in_distribtuion)
    '''
    y_true, y_dists = _check_kde_metrics_input(y_true, y_dists, frac)

    kde = KDE(**kde_kwargs)
    if progress_bar:
        likelihoods = np.array([kde.fit(y_dists[i]).evaluate(y_true[i]) for i in tqdm([*range(y_dists.shape[0])])])
    else:
        likelihoods =  np.array([kde.fit(y_dists[i]).evaluate(y_true[i]) for i in range(y_dists.shape[0])])

    return _fix_dist_1d(likelihoods)

# Cell
def _kde_quantile(y_true, y_dists, frac = 1.0, progress_bar = False, **kde_kwargs):
    '''
    fits a kde in a distribution and returns the quantile that a point in y_true belongs to in that distribution
    '''
    y_true, y_dists = _check_kde_metrics_input(y_true, y_dists, frac)

    kde = KDE(**kde_kwargs)
    if progress_bar:
        return _fix_one_dist_2d(np.array([kde.fit(y_dists[i]).cdf(y_true[i]) for i in tqdm([*range(len(y_dists))])]))
    else:
        return _fix_one_dist_2d(np.array([kde.fit(y_dists[i]).cdf(y_true[i]) for i in range(len(y_dists))]))

# Cell
def _quantile(y_true, y_dists):
    '''
    checks in which quantile lies y_true, given the predicted distribution
    y_true shape should be of shape (n_dists, n_samples ,n_dims)
    y_dists_should be of shape (n_dists, n_samples, n_dims)
    '''

    y_true = _assert_dim_3d(y_true)
    y_dists = _assert_dim_3d(y_dists)
    assert y_true.shape[0] == y_dists.shape[0], f'number of dists should be the same in both y_true and y_dists. got {y_true.shape[0]} and {y_dists.shape[0]}'

    values = []
    for i in range(y_true.shape[0]):
        values.append([])
        for j in range(y_true.shape[1]):
            values[i].append((y_true[i,j].T >= y_dists[i]).mean(axis = 0))

    return _fix_one_sample_2d(np.array(values))

# Cell
# CREATE EMPIRICAL DIST METHODS (WITH ADD NOISE IN SAMPLING OPTION AND ALL) <-----


class Empirical:
    '''
    empirical/histogram class
    '''

    def __init__(self, fit_frac=1):
        '''
        bins can be str (passed to np.histogram_bin_edges), n_bins (passed to np.histogram_bin_edges) or None
        '''
        assert 0 < fit_frac <= 1, 'fit_frac should be 0 < fit_frac <= 1'
        self.fit_frac = fit_frac
        return

    def fit(self, X, y=None, sample_weight=None):
        '''
        saves data into memory
        '''

        if len(X.shape) == 1:
            X = _fix_X_1d(X)
        assert len(X.shape) == 2, f'X expected to have 2 dimensions, got {len(X.shape)}'

        if sample_weight is None:
            self.data = X
            self.weights = None
        else:
            assert X.shape[0] == sample_weight.shape[0], f'''
            X and sample_weight must be the same size along dimension 0. got {X.shape[0]} and {sample_weight.shape[0]}'''
            self.data = X
            self.weights = sample_weight

        return self

    def sample(self, sample_size):
        '''
        alias for rvs
        '''
        return self.rvs(size = sample_size)

    def rvs(self, size):
        '''
        samples from self.data
        '''
        samples = sample_multi_dim(self.data, sample_size=sample_size, weights=self.weights, )
        return samples

    def rvs(self, size):
        '''
        samples from self.data
        alias for sample
        '''
        samples = sample_multi_dim(self.data, sample_size=size, weights=self.weights, )
        return samples

    def _reshape_X_and_dist(self, X, dist):
        X = np.array(X)
        if len(X.shape) == 1:
            X = _fix_X_1d(X)
        X = X.reshape(1, *X.shape)
        dist = dist.reshape(1, *dist.shape)
        return X, dist

    def pdf(self, X, inference_sample_size=1000, **pdf_kwargs):
        '''
        fits a kde and checks its likelihood to a data sample of dist
        '''
        dist = self.sample(inference_sample_size)
        X, dist = self._reshape_X_and_dist(X, dist)
        return _kde_likelihood(X, dist, **pdf_kwargs)

    def ppf(self, X, inference_sample_size=1000):
        '''
        returns the percent point function of X given a sample of distribution
        '''
        X = np.array(X)
        assert len(X.shape) == 1, f'X should have 1 dimension, got {len(X.shape)}'
        dist = self.sample(inference_sample_size)
        dist = _fix_one_dist_2d(dist)
        X = X.reshape(1, *X.shape)
        return _ppf(X, dist)[0, :, :]

    def cdf(self, X, inference_sample_size=1000):
        '''
        returns the cumulative distribution function of X given a sample of distribution along all dimensions
        '''
        X = np.array(X)
        dist = self.sample(inference_sample_size)
        X, dist = self._reshape_X_and_dist(X, dist)
        values = _quantile(X, dist)
        return values[0, :, :]

    def entropy(self, inference_sample_size = 1000, **entropy_kws):
        samples = self.sample(inference_sample_size)
        samples = samples.reshape(1,*samples.shape)
        return _kde_entropy(samples,**entropy_kws)

# Cell

class RandomVariable():
    '''
    A container for distribution objects
    '''

    def __init__(self, default_dist = 'empirical', calculate_likelihood = False, verbose = False, keep_samples = False):
        self._fitted_dists = {}
        self.log_likelihood = []
        self.default_dist = default_dist
        self.verbose = verbose
        self.keep_samples = keep_samples
        self._samples = None
        self.calculate_likelihood = calculate_likelihood
        return

    def _reset_fits(self,):
        self._fitted_dists = {}
        self.log_likelihood = []
        self._samples = None

    def __getitem__(self, item):
        if item == 'best':
            try:
                dist = self._handle_dist_arg(item)
                item = self._best_fit_alias
            except AttributeError:
                raise AttributeError('RandomVariable object has no "best" fit yet. Fit at least one density function through fit_dist method')

        return self._fitted_dists[item][0]

    def fit_new(self, data, dist = None, **fit_kwargs):
        '''
        fits given distributions
        creates alias `best` for dist with maximum likelihood
        '''
        if dist is None:
            dist = self.default_dist

        if dist.__class__ in [list,tuple,set]:
            pass
        elif dist.__class__ == str:
            dist = [dist]
        else:
            raise TypeError(f'dist should be str, list, tuple or set, not {dist.__class__}')

        self.n_dim = 1 if len(data.shape) == 1 else data.shape[-1]
        if self.keep_samples:
            self._samples = data

        self._fit_all(data ,dist, **fit_kwargs)

        return self

    def fit(self, data, dist = None, **fit_kwargs):
        self._reset_fits()
        self.fit_new(data, dist, **fit_kwargs)
        return self

    def _check_best(self):
        if self.calculate_likelihood:
            dists_aliases = list(self._fitted_dists)
            dists_arr = np.array([i[1] for i in self._fitted_dists.values()])
            best_fit_idx = np.argmax(dists_arr)
            self._best_fit_alias = dists_aliases[best_fit_idx]
        else:
            self._best_fit_alias = None

        return

    def _fit_all(self, data, dists, **fit_kwargs):
        #TODO: check for multiplicity in candidates aliases
        for candidate in ctqdm(dists, verbose = self.verbose):
            self._fit_dist(data, candidate, **fit_kwargs)
        return self

    def _fit_dist(self, data, dist, **fit_kwargs):
        '''
        fits a specified distribution through scipy.stats.rv_continuous.fit method
        '''
        alias, dist_name = self._handle_dist_names(dist)
        alias, dist_class = self._get_dist_from_name(alias, dist_name)
        if alias.lower() == 'best':
            raise ValueError('"best" cannot be an alias for a distribution. its internally assgined to the best fit dist')

        if alias == 'rv_histogram':

            hist = np.histogram(data, bins = 'auto')#len(np.unique(data)))
            dist = dist_class(hist)
            #make this step to optimize since log likelihiood estimation can be expensive
            if self.calculate_likelihood:
                log_likelihood = np.sum(np.log(dist.pdf(data)))
            else:
                log_likelihood = None

            self._fitted_dists = {**self._fitted_dists, **{alias:(dist,log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())

        elif not alias in ['kde','empirical']:
            if self.n_dim > 1:
                raise ValueError('rv_continuous distributions is only available for 1d distributions. Use "kde" dist instead.')
            params = dist_class.fit(data, **fit_kwargs)
            #make this step to optimize since log likelihiood estimation can be expensive
            if self.calculate_likelihood:
                log_likelihood = np.sum(np.log(dist_class.pdf(data,*params)))
            else:
                log_likelihood = None

            self._fitted_dists = {**self._fitted_dists, **{alias:(dist_class(*params),log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())
        else:
            #fit kws passed to constructor in sklearn fashion
            dist = dist_class(**fit_kwargs).fit(data)
            #make this step to optimize since log likelihiood estimation can be expensive
            if self.calculate_likelihood:
                log_likelihood = np.sum(np.log(dist.pdf(data)))
            else:
                log_likelihood = None

            self._fitted_dists = {**self._fitted_dists, **{alias:(dist,log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())

        #update 'best' alias
        self._check_best()
        return self


    def _get_dist_from_name(self, alias, dist_name):
        '''
        handles dist_names. if str tries to get an attribute from scipy.stats accordingly
        that is also instance of scipy.stats.rv_continuous
        '''
        if isinstance(dist_name,str):
            if dist_name.lower() == 'kde':
                alias = 'kde'
                return (alias, KDE)

            elif dist_name.lower() == 'empirical':
                alias = 'kde'
                return (alias, Empirical)

            elif dist_name in dir(stats):
                alias = dist_name
                return (alias, getattr(stats,dist_name))

            else:
                raise ValueError(f'dist must be a valid scipy.stats.rv_continuous subclass, not {getattr(stats,dist_name)}')

        elif isinstance(dist_name, stats.rv_continuous):
            return (alias, dist_name)
        else:
            raise ValueError(f'dist must be a valid scipy.stats.rv_continuous subclass or str, not {dist_name}')

    def _handle_dist_names(self, candidate_value):
        '''
        checks the inputs in elements of "candidates"
        returns a named tuple
        '''
        if isinstance(candidate_value, str):
            return candidate_value, candidate_value

        elif isinstance(candidate_value, tuple):

            if not len(candidate_value) == 2:
                raise ValueError(f'candidate named tuple must be of size 2, "{candidate_value}" has size {len(candidate_value)}')

            if not isinstance(candidate_value[0], str):
                raise ValueError(f'a candidate must be a str or named tuple (alias[str],<rv_continuous intance>), alias is of type {candidate_value[0].__class__}')

            else:
                return candidate_value

    def sample(self, sample_size, dist = 'best', **kwargs):
        '''
        alias for rvs
        '''
        return self.rvs(size = sample_size, dist = dist, **kwargs)

    def rvs(self, size, dist = 'best', **kwargs):
        '''
        sampling function
        '''
        dist = self._handle_dist_arg(dist)

        samples = self[dist].rvs(size = size, **kwargs)

        if len(samples.shape) == 1:
            samples = samples.reshape(*samples.shape,1)

        return samples

    def _fix_inference_data_input(self, data):
        if len(data.shape) == 1:
            data = data.reshape(-1,1)
        _assert_dim_2d(data)
        assert data.shape[1] == self.n_dim, f'Expected data to have shape (n_samples, n_dims({self.n_dim})). got (n_samples, n_dims({data.shape[1]})).'
        return data

    def _fix_inference_output(self, data):
        if len(data.shape) == 1:
            return data.reshape(-1,1)
        else:
            return data

    def cdf(self, data, dist = 'best', **cdf_kws):
        dist = self._handle_dist_arg(dist)

        #no need to fix in new ppf
        #data = self._fix_inference_data_input(data)
        samples = self[dist].cdf(data, **cdf_kws)
        return self._fix_inference_output(samples)

    def pdf(self, data, dist = 'best', **pdf_kws):
        dist = self._handle_dist_arg(dist)

        #no need to fix in new ppf
        #data = self._fix_inference_data_input(data)
        samples =  self[dist].pdf(data, **pdf_kws)
        return self._fix_inference_output(samples)

    def evaluate(self, data, dist = 'best', **evaluate_kws):
        '''alias for self.pdf'''
        return self.pdf(data, dist, **evaluate_kws)

    def predict(self, data, dist = 'best', **predict_kws):
        '''alias for self.pdf'''
        return self.pdf(data, dist, **predict_kws)

    def ppf(self, data, dist = 'best', **ppf_kws):
        '''
        percent point function
        '''
        dist = self._handle_dist_arg(dist)

        #no need to fix in new ppf
        #data = self._fix_inference_data_input(data)
        samples = self[dist].ppf(data)
        return self._fix_inference_output(samples)

    def entropy(self, dist = 'best', **entropy_kws):
        dist = self._handle_dist_arg(dist)
        return self[dist].entropy(**entropy_kws)

    def _handle_dist_arg(self, dist):

        if (self._best_fit_alias is None) and (dist == 'best') and (len(self._fitted_dists) > 1):
            raise ValueError(f'No likelihood value have been calculated, so a dist other than "best" should be specified in the arguments or calculate_likelihood should be set to True in constructor')
        if len(self._fitted_dists) == 1:
            dist = list(self._fitted_dists)[0]

        return dist

# Cell
#CREATE EMPIRICAL XXDIST CLASS (RV_HISTOGRAM IS TOO SLOW)

class CustomArray:
    '''
    An array that contains RandomVariable objects and facilitates method calls and getting attributes
    '''

    def __init__(self, data):
        ''' the constructor recieves a list of RandomVariable items'''
        self._data = np.array(data)

    @property
    def data(self,):
        return self._data

    def __getattr__(self, attr):
        '''
        Custom __getattr__ method
        '''
        attr_list = []
        for i in self.data:
            attr_list.append(getattr(i,attr))
        if all([callable(i) for i in attr_list]):
            return CustomArray(attr_list)
        else:
            return np.array(attr_list)


    def __call__(self, *args, broadcast_method = 'simple', **kwargs):
        '''
        broadcast_method can be called in two ways:
        simple: the same args and kwargs are applied to all the objects inside RVArray
        broadcast: for each (row) object in RVArray, the correspondent (same row) arg and kwarg is applied
        '''
        assert broadcast_method in ['simple','broadcast']

        if broadcast_method == 'simple':
            results = []
            for i in self.data:
                results.append(i(*args,**kwargs))

            if all([isinstance(i,np.ndarray) for i in results]):
                return np.array(results)
            else:
                return CustomArray(results)

        elif broadcast_method == 'broadcast':

            if args:
                args_lens_check = [len(arg) == len(self.data) for arg in args]
                assert all(args_lens_check)
            if kwargs:
                kwargs_lens_check = [len(arg) == len(self.data) for arg in kwargs.items()]
                assert all(kwargs_lens_check)

            #prepare args
            if args:
                _args = []
                for arg in args:
                    _args.append([val for val in arg])
                args = _args
            #prepare kwargs
            _kwargs = []
            if kwargs:
                _len = len(kwargs[list(kwargs)[0]])
                for i in range(_len):
                    kwargs_i = {}
                    for key in kwargs:
                        kwargs_i[key] = kwargs[key][i]
                    _kwargs.append(kwargs_i)
                kwargs = _kwargs

            #run
            if kwargs and args:
                results = []
                for i in range(len(self.data)):
                    results.append(self.data[i](*args[i],**kwargs[i]))

            elif kwargs and not args:
                results = []
                for i in range(len(self.data)):
                    results.append(self.data[i](*args,**kwargs[i]))

            elif not kwargs and args:
                results = []
                for i in range(len(self.data)):
                    results.append(self.data[i](*[arg[i] for arg in args],**kwargs))
            else:
                results = []
                for i in range(len(self.data)):
                    results.append(self.data[i](*args,**kwargs))

            #return values
            if all([isinstance(i,np.ndarray) for i in results]):
                return np.array(results)
            else:
                return CustomArray(results)


    def __getitem__(self, *args):

        if len(args) > 1:
            return CustomArray(self.data[args])
        else:
            if args[0].__class__ == str:
                return CustomArray([i[args[0]] for i in self.data])
            else:
                return self.data[args]

    def __repr__(self):
        return f'CustomArray({str(self.data)})'

    def _broadcastable_kwargs(self, kwargs):
        return {k:len(self.data)*[v] for k,v in kwargs.items()}

    def _broadcastable_args(self, args):
        return [len(self.data)*[v] for v in args]

    def _broadcastable_arg(self, arg):
        return len(self.data)*[arg]

class RVArray(CustomArray):
    '''
    A container containing RandomVariable objects. it allows for easily assessing methods and attributes
    from multiple RandomVariable objects simultaneously.
    Since its used for assessing methods of already fitted distributions, the `fit` method is disabled
    '''
    def __init__(self, rv_objects):
        #skip assertion allowing duck typing
        #assert all(isinstance(i, RandomVariable) for i in rv_objects), 'All rv_objects passed to cosntructor should be instances of skdensity.core.random_variavle.RandomVariable'
        super().__init__(rv_objects)
        return

    def fit_new(self, data, dist = None, **dist_kwargs):
        '''
        Same as RandomVariable.fit_new.
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise
        '''
        data = np.array(data)

        #no broadcasting case
        if len(data.shape) in (1,2):
            super().__getattr__('fit_new')(data, dist, broadcast_method = 'simple',**dist_kwargs)
            return self
        #broadcasting case
        dist_kwargs = self._broadcastable_kwargs(dist_kwargs)
        dist = self._broadcastable_arg(dist)
        super().__getattr__('fit_new')(data, dist, broadcast_method = 'broadcast',**dist_kwargs)
        return self

    def fit(self, data, dist = None, **dist_kwargs):
        '''
        Same as RandomVariable.fit
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise.
        '''
        data = np.array(data)

        #no broadcasting case
        if len(data.shape) in (1,2):
            super().__getattr__('fit')(data, dist, broadcast_method = 'simple',**dist_kwargs)
            return self
        #broadcasting case
        dist_kwargs = self._broadcastable_kwargs(dist_kwargs)
        dist = self._broadcastable_arg(dist)
        super().__getattr__('fit')(data, dist, broadcast_method = 'broadcast',**dist_kwargs)
        return self

    def entropy(self, dist = 'best', **entropy_kws):
        '''
        Same as RandomVariable.entropy
        '''
        return super().__getattr__('entropy')(dist, broadcast_method = 'simple', **entropy_kws)

    def ppf(self, data, dist = 'best', **ppf_kws):
        '''
        Same as RandomVariable.ppf
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise.
        '''
        data = np.array(data)

        #no broadcasting case
        if len(data.shape) in (1,2):
            return super().__getattr__('ppf')(data, dist, broadcast_method= 'simple',**ppf_kws)

        ppf_kws = self._broadcastable_kwargs(ppf_kws)
        dist = self._broadcastable_arg(dist)
        return super().__getattr__('ppf')(data, dist, broadcast_method='broadcast', **ppf_kws)

    def predict(self, data, dist = 'best', **predict_kws):
        '''
        Same as RandomVariable.predict
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise.
        '''
        data = np.array(data)
        #no broadcasting case
        if len(data.shape) in (1,2):
            return super().__getattr__('predict')(data, dist, broadcast_method= 'simple',**predict_kws)

        dist = self._broadcastable_arg(dist)
        predict_kws = self._broadcastable_kwargs(predict_kws)
        return super().__getattr__('predict')(data, dist, broadcast_method = 'broadcast',**predict_kws)

    def evaluate(self, data, dist = 'best', **evaluate_kws):
        '''
        Same as RandomVariable.evaluate
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise.
        '''
        data = np.array(data)
        #no broadcasting case
        if len(data.shape) in (1,2):
            return super().__getattr__('evaluate')(data, dist, broadcast_method= 'simple',**evaluate_kws)

        dist = self._broadcastable_arg(dist)
        evaluate_kws = self._broadcastable_kwargs(evaluate_kws)
        return super().__getattr__('evaluate')(data, dist, broadcast_method = 'broadcast',**evaluate_kws)

    def pdf(self, data, dist = 'best', **pdf_kws):
        '''
        Same as RandomVariable.pdf
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise.
        '''
        data = np.array(data)
        #no broadcasting case
        if len(data.shape) in (1,2):
            return super().__getattr__('pdf')(data, dist, broadcast_method= 'simple',**pdf_kws)

        dist = self._broadcastable_arg(dist)
        pdf_kws = self._broadcastable_kwargs(pdf_kws)
        return super().__getattr__('pdf')(data, dist, broadcast_method = 'broadcast',**pdf_kws)


    def cdf(self, data, dist = 'best', **cdf_kws):
        '''
        Same as RandomVariable.cdf
        can be applied in a broadcasted manner if shape (n_dists, n_samples, n_dims),
        No broadcasting otherwise.
        '''
        data = np.array(data)

        #no broadcasting case
        if len(data.shape) in (1,2):
            return super().__getattr__('cdf')(data, dist, broadcast_method = 'simple',**cdf_kws)
        #broadcasting case
        cdf_kws = self._broadcastable_kwargs(cdf_kws)
        dist = self._broadcastable_arg(dist)
        return super().__getattr__('cdf')(data, dist, broadcast_method = 'broadcast',**cdf_kws)

    def rvs(self, size = 1, dist = 'best', **kwargs):
        '''
        Same as RandomVariable.rvs
        '''
        return super().__getattr__('rvs')(size, dist, broadcast_method = 'simple',**kwargs)

    def sample(self, sample_size = 1, dist = 'best', **kwargs):
        '''
        Same as RandomVariable.sample
        '''
        return super().__getattr__('sample')(sample_size, dist, broadcast_method = 'simple',**kwargs)

