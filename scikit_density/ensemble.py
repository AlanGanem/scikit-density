# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/01_ensemble.ipynb (unless otherwise specified).

__all__ = ['expected_likelihood', 'inverese_log_node_var', 'datapoint_pdf', 'datapoint_gaussian_likelihood',
           'AVALIBLE_NODE_AGG', 'AVALIBLE_DATAPOINT_WEIGHT_FUNC', 'EnsembleTreesDensityMixin', 'SimilarityTreeEnsemble',
           'BaggingDensityEstimator', 'AdaBoostDensityEstiamtor']

# Cell
from warnings import warn
from functools import partial
import copy
from tqdm.notebook import tqdm

import numpy as np
import pandas as pd
from sklearn import ensemble
from sklearn.preprocessing import OneHotEncoder, normalize, QuantileTransformer, FunctionTransformer
from sklearn.multioutput import MultiOutputRegressor
from sklearn.utils.fixes import _joblib_parallel_args
import scipy
from joblib import Parallel, delayed


from .utils import cos_sim_query, sample_multi_dim, ctqdm, DelegateEstimatorMixIn
from .core.random_variable import KDE

# Cell


#node quality functions
def expected_likelihood(node_data, sample_size = 100):
    kde = KDE().fit(node_data)
    return np.mean(kde.pdf(kde.rvs(size = sample_size)))

def inverese_log_node_var(node_data):
    return 1/np.log1p(np.var(node_data))


# datapoint-node functions
def datapoint_pdf(node_data):
    return KDE().fit(node_data).pdf(node_data)

def datapoint_gaussian_likelihood(node_data):
    std = node_data.std()
    mean = node_data.mean()
    z = (node_data - mean)/std
    return 1/(std*np.pi**(1/2))*np.exp(-1/2*z**2)


AVALIBLE_NODE_AGG = {
    'expected_likelihood':expected_likelihood,
    'inverse_log_variance':inverese_log_node_var,
}

AVALIBLE_DATAPOINT_WEIGHT_FUNC = {
    'kde_likelihood': datapoint_pdf,
    'gaussian_likelihood': datapoint_gaussian_likelihood
}

# Cell
class EnsembleTreesDensityMixin():

    '''Base Class containing important methods for building Naive and Similarity Density Tree estimators'''

    def _fit_leaf_node_matrix(self, X, y, node_rank_func, node_data_rank_func):
        nodes_array = self.estimator.apply(X)
        encoder = OneHotEncoder()
        leaf_node_matrix = encoder.fit_transform(nodes_array)
        leaf_node_matrix = self._make_weighted_query_space(y, leaf_node_matrix, node_data_rank_func)

        self.leaf_node_weights = self._calculate_node_weights(y, leaf_node_matrix, node_rank_func)
        self.leaf_node_matrix = leaf_node_matrix
        self.encoder = encoder
        return self

    def _transform_query_matrix(self, X):

        return  self._make_weighted_query_vector(
            agg_node_weights = self.leaf_node_weights,
            node_matrix = self.encoder.transform(self.estimator.apply(X)))


    def _query_idx_and_sim(self, X, n_neighbors, lower_bound, beta, gamma):
        idx, sim = cos_sim_query(
            self._transform_query_matrix(X), self.leaf_node_matrix, n_neighbors=n_neighbors,
            lower_bound=lower_bound, beta = beta, gamma = gamma)
        return idx, sim

    def _similarity_sample(self, X, sample_size = 100, weights = None, n_neighbors = 10,
                           lower_bound = 0.0, alpha = 1, beta = 0, gamma = 0):

        idx, sim = self._query_idx_and_sim(X ,n_neighbors=n_neighbors, lower_bound=lower_bound,beta = beta, gamma = gamma)
        idx, sim = np.array(idx), np.array(sim)

        p = self._handle_sample_weights(weight_func = weights, sim = sim, alpha = alpha)
        samples = []
        for i in range(len(idx)):
            samples.append(self.y_[sample_multi_dim(idx[i], size = sample_size, weights = p[i], axis = 0)])

            #sampled_idxs = np.random.choice(idx[i], size = sample_size, replace = True, p = p[i])
            #samples.append(self.y_[sampled_idxs])

        return np.array(samples)

    def _similarity_empirical_pdf(self, X, weights, n_neighbors, lower_bound, alpha, beta, gamma):

        idx, sim = cos_sim_query(
            self._transform_query_matrix(X),
            self.leaf_node_matrix,
            n_neighbors=n_neighbors,
            lower_bound=lower_bound,
            beta = beta,
            gamma = gamma)
        p = self._handle_sample_weights(weight_func = weights, sim = sim, alpha = alpha)
        return np.array([self.y_[i] for i in idx]), p

    def _sim_predict(self, X, weights, n_neighbors, lower_bound, alpha, beta, gamma):
        '''wieghts must be None or callable that operates on similarity values'''

        values, weights = self._similarity_empirical_pdf(X, weights, n_neighbors, lower_bound, alpha, beta, gamma)

        y_multioutput = (len(values.shape) - 1) > 1
        if y_multioutput:
            y_dim = values.shape[-1]
            weights = np.repeat(weights, y_dim, axis = -1).reshape(*weights.shape, y_dim)
            return np.average(values, weights = weights, axis = -2)
        else:
            return np.average(values, weights = weights, axis = -1)

    def _calculate_node_weights(self, y, node_matrix, node_rank_func):
        '''
        calculates node weights that maultiplies the query space matrix, in order to make some nodes more relevant
        according to some target data node agg metric.
        input should be a list containing array of node samples as each one of its elements
        '''

        if not node_rank_func is None:
            node_data_generator = self._make_node_data_generator(y, node_matrix)
            # cannot call in a vectorized fashion because data from nodes may have different sizes
            if callable(node_rank_func):
                pass
            else:
                node_rank_func = AVALIBLE_NODE_AGG[node_rank_func]

            node_weights = Parallel(n_jobs=-1, verbose=0,
                               **_joblib_parallel_args(prefer="threads"))(
                delayed(node_rank_func)(X)
                for X in node_data_generator)
        else:
            node_weights = np.ones(node_matrix.shape[1])

        return np.array(node_weights)

    def _calculate_node_datapoint_weights(self, y, node_matrix, node_data_rank_func):
        '''
        Calculates node-datapoint(y values) weights. higher values meansa datapoint "belongs tighter"
        to that point and is more loleky to be sampled when that node is reached. some cases of node-datapount wieghts
        could be the likelihood of that point given the node pdf, or some sort of median/mean deviance from point to node samples
        '''
        #
        if callable(node_data_rank_func):
            pass
        else:
            node_data_rank_func = AVALIBLE_DATAPOINT_WEIGHT_FUNC[node_data_rank_func]

        node_data_generator = self._make_node_data_generator(y, node_matrix)
        #datapoint_node_weights = Parallel(n_jobs=1, verbose=0,
        #                   **_joblib_parallel_args(prefer="threads"))(
        #    delayed(node_data_rank_func)(X)
        #    for X in node_data_generator)

        datapoint_node_weights = [node_data_rank_func(node_data) for node_data in node_data_generator]
        return datapoint_node_weights

    def _make_node_data_generator(self, y, node_matrix):
        nodes = node_matrix.tocoo().col
        idxs = node_matrix.tocoo().row
        return (y[idxs[nodes == i]] for i in tqdm([*range(node_matrix.shape[1])]))

    def _handle_sample_weights(self, weight_func, sim, alpha):
        '''
        sampling wights should sum to 1, since its a sampling probability
        '''
        if weight_func is None:
            return np.array([normalize((i**alpha).reshape(1,-1) + 1e-9, norm = 'l1').flatten() for i in sim])
        else:
            return np.array([normalize((weight_func(i)).reshape(1,-1) + 1e-9, norm = 'l1').flatten() for i in sim])

    def _make_weighted_query_vector(self, agg_node_weights, node_matrix):
        '''
        multiplies elements of query vector by their respective weights
        the greater the weights, the better the "quality" of the nodes
        '''
        assert isinstance(node_matrix, scipy.sparse.csr_matrix), 'input should be instance of scipy.sparse.csr_matrix'
        node_matrix.data = node_matrix.data*np.take(agg_node_weights, node_matrix.indices)
        return node_matrix

    def _make_weighted_query_space(self, y, node_matrix, node_data_rank_func = None):
        '''
        query space is the leaf_node_matrix multiplied by node_data_weights
        the greater the value in the matrix, the better the "quality" of that data point
        '''
        assert isinstance(node_matrix, scipy.sparse.csr_matrix), 'input should be instance of scipy.sparse.csr_matrix'

        if not node_data_rank_func is None:
            # datapoint_node_weights multiplication (columns)
            #make copy
            node_matrix = copy.deepcopy(node_matrix)
            #cast to csc to make .data order columnwise
            node_matrix = node_matrix.tocsc()
            datapoint_node_weights = self._calculate_node_datapoint_weights(y, node_matrix, node_data_rank_func)
            node_matrix.data = node_matrix.data*np.concatenate(datapoint_node_weights)
            #convert back to csr
            node_matrix = node_matrix.tocsr()
        else:
            pass

        return node_matrix


#WEIGHTS ARE TO SMALL, SCALE BEFORE MULTIPLY


# Cell

class SimilarityTreeEnsemble(EnsembleTreesDensityMixin):

    def __init__(self, estimator, alpha = 1, beta = 1, gamma = 1, node_rank_func = 'expected_likelihood',
                 node_data_rank_func = 'kde_likelihood',n_neighbors = 30, lower_bound = 0.0):

        self.estimator = estimator
        self.n_neighbors = n_neighbors
        self.lower_bound = lower_bound
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.node_rank_func = node_rank_func
        self.node_data_rank_func = node_data_rank_func
        return

    def __repr__(self):
        return self.__class__.__name__

    def fit(self, X, y = None):
        self.estimator.fit(X,y)
        self._fit_leaf_node_matrix(
            X, y, node_rank_func = self.node_rank_func, node_data_rank_func = self.node_data_rank_func)# <- MAKE NODE WIEGHTED VERSION
        self.y_ = y
        return self

    def sample(self, X, sample_size = 10, weights = None, n_neighbors = None,
               lower_bound = None, alpha = None, beta = None, gamma = None):
        '''wieghts should be callable (recieves array returns array of same shape) or None'''
        n_neighbors, lower_bound, alpha, beta, gamma = self._handle_similarity_sample_parameters(n_neighbors, lower_bound,
                                                                                    alpha, beta, gamma)

        return super()._similarity_sample(
            X = X, sample_size = sample_size, weights = weights, n_neighbors = n_neighbors,
            lower_bound = lower_bound, alpha = alpha, beta = beta, gamma = gamma
        )

    def sim_predict(self, X, weights = None, n_neighbors = None, lower_bound = None, alpha = None, beta = None, gamma = None):
        n_neighbors, lower_bound, alpha, beta, gamma = self._handle_similarity_sample_parameters(n_neighbors, lower_bound, alpha, beta, gamma)
        return self._sim_predict(X, weights, n_neighbors, lower_bound, alpha, beta, gamma)

    def _handle_similarity_sample_parameters(self, n_neighbors, lower_bound, alpha, beta, gamma):

        if n_neighbors is None:
            n_neighbors = self.n_neighbors
        if lower_bound is None:
            lower_bound = self.lower_bound
        if alpha is None:
            alpha = self.alpha
        if beta is None:
            beta = self.beta
        if alpha is None:
            gamma = self.gamma

        return n_neighbors, lower_bound, alpha, beta, gamma

# Cell
def _parallel_predict_regression(estimators, estimators_features, X):
    """Private function used to compute predictions within a job."""
    return np.array([estimator.predict(X[:, features])
               for estimator, features in zip(estimators,
                                              estimators_features)])

class BaggingDensityEstimator(ensemble.BaggingRegressor):

    def sample(self, X, sample_size = 10, weights = None):
        idxs = self._sample_idxs(X, sample_size, weights)
        predictions = self._predict_all_estimators(X)
        return self._sample_from_idxs(predictions, idxs)

    def _predict_all_estimators(self, X):
        ensemble._bagging.check_is_fitted(self)
        # Check data
        X = ensemble._bagging.check_array(
            X, accept_sparse=['csr', 'csc'], dtype=None,
            force_all_finite=False
        )

        # Parallel loop
        n_jobs, n_estimators, starts = ensemble._bagging._partition_estimators(self.n_estimators,
                                                             self.n_jobs)

        all_y_hat = ensemble._bagging.Parallel(n_jobs=n_jobs, verbose=self.verbose)(
            ensemble._bagging.delayed(_parallel_predict_regression)(
                self.estimators_[starts[i]:starts[i + 1]],
                self.estimators_features_[starts[i]:starts[i + 1]],
                X)
            for i in range(n_jobs))

        predictions = np.swapaxes(all_y_hat[0], 0, 1)
        return predictions

    def _sample_from_idxs(self, predictions, idxs):
        return np.array([predictions[i][idxs[i]] for i in range(len(idxs))])

    def _sample_idxs(self, X, sample_size, weights):
        '''Sample indxs according to estimator weights'''
        return [np.random.choice([*range(self.n_estimators)], size = sample_size, p = weights) for i in range(X.shape[0])]


# Cell
class AdaBoostDensityEstiamtor(ensemble.AdaBoostRegressor):

    def sample(self, X, sample_size = 10, weights = 'boosting_weights'):
        idxs = self._sample_idxs(X, sample_size, weights)
        predictions = self._predict_all_estimators(X)
        return self._sample_from_idxs(predictions, idxs)

    def _predict_all_estimators(self, X):
        return np.array([est.predict(X) for est in self.estimators_[:len(self.estimators_)]]).T

    def _sample_from_idxs(self, predictions, idxs):
        return np.array([predictions[i][idxs[i]] for i in range(len(idxs))])

    def _sample_idxs(self, X, sample_size, weights):
        '''Sample indxs according to estimator weights'''

        if weights is None:
            weights = np.ones(sample_size)[:len(self.estimators_)]
            weights /= weights.sum()

        elif weights == 'boosting_weights':
            weights = self.estimator_weights_[:len(self.estimators_)]
            weights /= weights.sum()
        else:
            weights = self.estimator_weights_[:len(self.estimators_)]*weights
            weights /= weights.sum()

        idxs = [np.random.choice([*range(len(self.estimators_))], size = sample_size, p = weights) for i in range(X.shape[0])]
        return idxs

