# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/02_core.random_variable.ipynb (unless otherwise specified).

__all__ = ['identity_func', 'agg_smallest_distance', 'KDE', 'IDENTITY_TRANSFORMER', 'RandomVariable', 'RVArray']

# Cell
from functools import partial
from warnings import warn

import scipy
import scipy.stats as stats
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import QuantileTransformer, FunctionTransformer
from sklearn.neighbors import KernelDensity
from sklearn.decomposition import PCA, KernelPCA

import KDEpy as kdepy
import awkde

from ..utils import cos_sim_query, sample_multi_dim, ctqdm, DelegateEstimatorMixIn, _vector_1d_to_matrix,_assert_dim_3d

# Cell

#Identity transformer in case space_transformer is None
def identity_func(x):
    return x

IDENTITY_TRANSFORMER = FunctionTransformer(
    func = identity_func,
    inverse_func = identity_func,
    validate=False,
    accept_sparse=True,
    check_inverse=True,
    kw_args=None,
    inv_kw_args=None,
)

def agg_smallest_distance(data, agg_func = np.mean):
    '''
    returns the agregate (defined by agg_func) distance of each point and their closest neighbor
    recieves array of shape (n_dists,n_samples, n_dims) and reutrns array of shape (n_dists, n_dims)
    '''
    _assert_dim_3d(data)
    data = np.sort(data, axis = 1)
    diff = np.diff(data, axis = 1)
    results = agg_func(diff, axis = 1)
    return results

class KDE():

    AVALIBLE_BW_METHODS = ['ISJ', 'scott', 'silverman', 'mean_distance', 'std_distance', 'median_distance']

    def __init__(self, bw = 'std_distance', space_transformer = PCA, implementation = 'sklearn', kde_kws = {}, st_kws = {}):

        if bw.__class__ == str:
            assert bw in self.AVALIBLE_BW_METHODS, f"if str, bw should be one of {self.AVALIBLE_BW_METHODS}, not {bw}"
        if not isinstance(bw,(str, float, np.float64, np.float32, np.float)):
            raise TypeError(f'bw should be str or float, not {bw.__class__}')
        self.bw = bw
        self._space_transformer = space_transformer if not space_transformer is None else IDENTITY_TRANSFORMER
        self.kde_kws = kde_kws
        self.st_kws = st_kws
        if not implementation in ['scipy','sklearn','awkde']:
            raise ValueError(f'implementation should be one of ["sklearn","scipy","awkde"], not {implementation}')

        self.implementation = implementation

    def _check_X_2d(self,X):
        X = np.array(X)
        #reshape if shape == (n_samples,)
        X = X if len(X.shape) > 1 else X.reshape(-1,1)
        return X

    def _check_input_dims_match(self, X):
        if X.shape[-1] != self.n_dim:
            raise ValueError(f'X dimensions space should be the same size as fitted distribution ({self.n_dim}), got {X.shape[-1]} instead')

    def _get_bw_each_dim(self, X, bw_method):
        if bw_method in ['ISJ', 'scott', 'silverman']:
            return np.array([kdepy.FFTKDE(bw = bw_method).bw(X[:,i:i+1]) for i in range(X.shape[-1])])
        elif bw_method == 'mean_distance':
            return np.array([agg_smallest_distance(X[:,i].reshape(1,X.shape[0],1), np.mean) for i in range(X.shape[-1])])
        elif bw_method == 'median_distance':
            return np.array([agg_smallest_distance(X[:,i].reshape(1,X.shape[0],1), np.median) for i in range(X.shape[-1])])
        elif bw_method == 'std_distance':
            return np.array([agg_smallest_distance(X[:,i].reshape(1,X.shape[0],1), np.std) for i in range(X.shape[-1])])

    def fit(self, X, y = None, sample_weight = None):
        X = self._check_X_2d(X)
        #fit and transform X with manifold learner (self.space_transformer)
        if isinstance(self._space_transformer, type):
            self._space_transformer = self._space_transformer(**{**self.st_kws, **{
                'n_components':X.shape[-1], 'whiten':True}})

        X = self._space_transformer.fit_transform(X)
        # calculate bw
        if self.bw.__class__ == str:
            bw = self._get_bw_each_dim(X, self.bw)
            bw = np.sqrt(np.sum(bw**2))
        else:
            warn('passing a float value for bw is not recomended since X will be transformed by space_transformer before fitting and bw value may not make sence in new trnasformed space')
            bw = self.bw

        #kde
        if self.implementation == 'sklearn':
            self.estimator = KernelDensity(**{**{'bandwidth':bw},**self.kde_kws}).fit(X, y, sample_weight = sample_weight)
        elif self.implementation == 'scipy':
            self.estimator = stats.gaussian_kde(X.T, bw_method = bw)
        elif self.implementation == 'awkde':
            self.estimator = awkde.GaussianKDE(**{**{'glob_bw':bw},**self.kde_kws})
            self.estimator.fit(X = X, weights = sample_weight)
        else: raise ValueError(f'self.implementation should be one of ["sklearn","scipy","awkde"], not {self.implementation}')

        self._transformed_bw_value = bw
        self.n_dim = X.shape[-1]
        return self

    def evaluate(self, data):
        data = self._check_X_2d(data)
        #transform input
        data = self._space_transformer.transform(data)
        self._check_input_dims_match(data)
        #get likelihoods
        if self.implementation == 'sklearn':
            likelihood = np.exp(self.estimator.score_samples(data))
        elif self.implementation == 'scipy':
            likelihood = self.estimator.pdf(data.T)
        elif self.implementation == 'awkde':
            likelihood = self.estimator.predict(data)
        else: raise ValueError(f'self.implementation should be one of ["sklearn","scipy","awkde"], not {self.implementation}')

        return likelihood

    def predict(self, X):
        return self.evaluate(X)

    def pdf(self, data):
        return self.evaluate(data)

    def rvs(self, sample_size = 1, random_state = None):
        if self.implementation == 'sklearn':
            samples = self.estimator.sample(n_samples = sample_size, random_state = random_state)
        elif self.implementation == 'scipy':
            samples = self.estimator.resample(sample_size, random_state).T
        elif self.implementation == 'awkde':
            samples = self.estimator.sample(n_samples = sample_size, random_state = random_state)
        else: raise ValueError(f'self.implementation should be one of ["sklearn","scipy","awkde"], not {self.implementation}')
        #inverse transform samples
        samples = self._space_transformer.inverse_transform(samples)
        return samples

    def sample(self, sample_size = 1, random_state = None):
        return self.rvs(sample_size, random_state)

    def entropy(self, sample_size = 100):
        return np.mean(-np.log2(self.evaluate(self.rvs(sample_size = sample_size))))

    def cdf(self, data, sample_size = 100):
        #estimate using sampling and QuantileTransformer since integration is too costly
        samples = self.sample(sample_size = sample_size)
        return QuantileTransformer(n_quantiles = min(1000,samples.shape[0])).fit(samples).transform(data)

    def ppf(self, data, sample_size = 100):
        #estimate using sampling and QuantileTransformer since integration is too costly
        data = np.array(data)
        assert (data.min() >= 0) and (data.max() <= 1), 'data contains values <= 0 or >= 1'
        samples = self.sample(sample_size = sample_size)
        return QuantileTransformer(n_quantiles = min(1000,samples.shape[0])).fit(samples).inverse_transform(data)

    def _make_conditioning_grid(self, condition_dict = {}, resolution = None):
        samples, likelihood = self.sample(1000) #estimate min and max intervals
        argsrt = np.argsort(likelihood)[::-1]
        likelihood_msk = likelihood[argsrt].cumsum() < 0.99*likelihood.sum()
        likelihood_msk = argsrt[likelihood_msk]
        #ignore points with low likelihood
        grid_min, grid_max = samples[likelihood_msk].min(axis = 0), samples[likelihood_msk].max(axis = 0)
        dim_grid = []
        for dim in range(grid_min.shape[0]):
            dim_min, dim_max = grid_min[dim], grid_max[dim]
            if not dim in condition_dict:
                dim_grid.append(np.linspace(dim_min,dim_max, resolution))
            else:
                dim_grid.append(np.linspace(condition_dict[dim],condition_dict[dim], resolution))
        return np.array(dim_grid).T


# Cell

class RandomVariable():
    '''
    A container for distribution objects
    '''

    @classmethod
    def from_weights(cls, values, weights, n_samples = 100):
        data = np.random.choice(values, size = n_samples, p = weights)
        return cls.__init__(data,)

    def __init__(self, data, verbose = False):
        self.samples = data
        self.n_dim = 1 if len(data.shape) == 1 else data.shape[-1]
        self._fitted_dists = {}
        self.log_likelihood = []
        self.verbose = False
        return

    def __getitem__(self, item):
        if item == 'best':
            try:
                item = self._best_fit_alias
            except AttributeError:
                raise AttributeError('RandomVariable object has no "best" fit yet. Fit at least one density function through fit_dist method')

        return self._fitted_dists[item][0]

    #def __repr__(self):
    #    return f'RandomVariable({str(self.samples)})'

    def fit_best(self, candidates = ['norm','halfnorm','lognorm']):
        #fit all and make alias for best fit
        self._fit_all(self.samples, candidates)
        return self

    def _fit_ecdf(self, data, **ecdf_kwargs):
        self.ecdf = QuantileTransformer(**ecdf_kwargs).fit(data)
        return self

    def fit_dist(self, dist, **dist_kwargs):
        return self._fit_dist(self.samples,dist, **dist_kwargs)

    def _check_best(self):
        dists_aliases = list(self._fitted_dists)
        dists_arr = np.array([i[1] for i in self._fitted_dists.values()])
        best_fit_idx = np.argmax(dists_arr)
        self._best_fit_alias = dists_aliases[best_fit_idx]
        return

    def _fit_all(self, data, candidates):
        #TODO: check for multiplicity in candidates aliases
        for candidate in ctqdm(candidates, verbose = self.verbose):
            self.fit_dist(candidate)
        return self

    def _fit_dist(self, data, dist, **dist_kwargs):
        '''
        fits a specified distribution through scipy.stats.rv_continuous.fit method
        '''
        alias, dist_name = self._handle_dist_names(dist)
        alias, dist_class = self._get_dist_from_name(alias, dist_name)
        if alias.lower() == 'best':
            raise ValueError('"best" cannot be an alias for a distribution. its internally assgined to the best fit dist')
        if alias != 'kde':
            if self.n_dim > 1:
                raise ValueError('rv_continuous distributions is only available for 1d distributions. Use "kde" dist instead.')
            params = dist_class.fit(data)
            log_likelihood = np.log(np.product(dist_class.pdf(data,*params)))
            self._fitted_dists = {**self._fitted_dists, **{alias:(dist_class(*params),log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())
        else:
            dist = dist_class(**dist_kwargs).fit(data)
            log_likelihood = np.log(np.product(dist.pdf(data)))
            self._fitted_dists = {**self._fitted_dists, **{alias:(dist,log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())

        #update 'best' alias
        self._check_best()
        return self


    def _get_dist_from_name(self, alias, dist_name):
        '''
        handles dist_names. if str tries to get an attribute from scipy.stats accordingly
        that is also instance of scipy.stats.rv_continuous
        '''
        if isinstance(dist_name,str):
            if isinstance(getattr(stats,dist_name), stats.rv_continuous):
                alias = dist_name
                return (alias, getattr(stats,dist_name))
            elif dist_name.lower() == 'kde':
                alias = 'kde'
                return (alias, KDE)
            else:
                raise ValueError(f'dist must be a valid scipy.stats.rv_continuous instance, not {getattr(stats,dist_name)}')

        elif isinstance(dist_name, stats.rv_continuous):
            return (alias, dist_name)
        else:
            raise ValueError(f'dist must be a valid scipy.stats.rv_continuous instance or str, not {dist_name}')

    def _handle_dist_names(self, candidate_value):
        '''
        checks the inputs in elements of "candidates"
        returns a named tuple
        '''
        if isinstance(candidate_value, str):
            return candidate_value, candidate_value

        elif isinstance(candidate_value, tuple):

            if not len(candidate_value) == 2:
                raise ValueError(f'candidate named tuple must be of size 2, "{candidate_value}" has size {len(candidate_value)}')

            if not isinstance(candidate_value[0], str):
                raise ValueError(f'a candidate must be a str or named tuple (alias[str],<rv_continuous intance>), alias is of type {candidate_value[0].__class__}')

            else:
                return candidate_value

    def sample(self, sample_size, dist = 'empirical', **kwargs):
        if dist == 'empirical':
            sampled_idxs = np.random.choice([*range(self.samples.shape[0])], size = sample_size, **kwargs)
            return self.samples[sampled_idxs]
        else:
            return self[dist].rvs(size = size, **kwargs)

    def cdf(self, data, dist = 'empirical'):
        if dist == 'empirical':
            return self.ecdf.transform(data)
        else:
            return self[dist].cdf(data)

    def pdf(self, data, dist = 'best'):
        if dist == 'empirical':
            raise ValueError('empirical quantile distribution has no pdf definition')
        else:
            return self[dist].pdf(data)

    def ppf(self, data, dist = 'empirical'):
        if dist == 'empirical':
            return self.ecdf.inverse_transform(data)
        else:
            return self[dist].ppf(data)

    def entropy(self, dist):
        return self[dist].entropy()


# Cell
class RVArray:
    '''
    An array that contains RandomVariable objects and facilitates method calls and getting attributes
    '''
    def __init__(self, data):
        ''' the constructor recieves a list of RandomVariable items'''
        self.data = np.array(data)

    def __getattr__(self, attr):
        attr_list = [getattr(i,attr) for i in self.data]
        return RVArray(attr_list)

    def __call__(self, *args, **kwargs):
        results = [i(*args,**kwargs) for i in self.data]
        if all([isinstance(i,np.ndarray) for i in results]):
            return np.array(results)
        else:
            return RVArray(results)

    def __getitem__(self, *args):

        if len(args) > 1:
            return RVArray(self.data[args])
        else:
            if args[0].__class__ == str:
                return RVArray([i[args[0]] for i in self.data])
            else:
                return self.data[args]

    def __repr__(self):
        return f'RVArray({str(self.data)})'