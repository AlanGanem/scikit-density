# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/02_core.random_variable.ipynb (unless otherwise specified).

__all__ = ['KDE', 'RandomVariable', 'RVArray']

# Cell
from functools import partial

import scipy.stats as stats
import numpy as np
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import QuantileTransformer

import KDEpy as kdepy

from ..utils import cos_sim_query, sample_multi_dim, ctqdm, DelegateEstimatorMixIn

# Cell
import awkde
import KDEpy as kdepy

class KDE():

    def __init__(self, kernel = 'gaussian', bw = 'ISJ', alpha = 0.5, implementation = 'auto'):
        self.kernel = kernel
        self.bw = bw
        self.alpha = alpha
        self.implementation = implementation

    def _check_X_2d(self,X):
        X = np.array(X)
        #reshape if shape == (n_samples,)
        X = X if len(X.shape) > 1 else X.reshape(-1,1)
        return X

    def _check_input_dims_match(self, X):
        if X.shape[-1] != self.n_dim:
            raise ValueError(f'X dimensions space should be the same size as fitted distribution ({self.n_dim}), got {X.shape[-1]} instead')

    def fit(self, X, y = None):
        X = self._check_X_2d(X)
        #grid_interval is the min and max values for each dimension. shape (n_dim, 2 -> (min,max))
        self.grid_interval = np.array([X.min(axis = 0),X.max(axis = 0)]).T

        #check if n_dim is lower equal to 10
        n_dim = X.shape[-1]
        self.n_dim = n_dim
        if n_dim > 10:
            raise ValueError(f'dimension of distribution should be lower or equal to 10. got {n_dim} dimensions instead')

        # resolution is such that grid sampling doesn't exceeds 10000 points
        self.kde_resolution = max(2,int(10000**(1/n_dim)))
        if self.implementation == 'auto':
            #select kde implementation according to scale
            # FFT is faster but its slower to evalute due to grid evaluation which scales as a power of grid resolution
            # Tiping point is arround 10^2.5 (approx 300) samples for 1D distributions
            if X.shape[0] > 300:
                # find marginal bws for multidim data
                bw = [kdepy.FFTKDE(bw = self.bw).bw(X[:,i:i+1]) for i in range(X.shape[-1])]
                self.estimator = kdepy.FFTKDE(kernel = self.kernel, bw = bw, )
            else:
                bw = self.bw if not self.bw == 'ISJ' else 'silverman'
                self.estimator = awkde.GaussianKDE(glob_bw = bw, alpha = self.alpha, )

        # if not auto then run 'adaptative' or 'FFTKDE'
        elif self.implementation == 'adaptative':
            self.estimator = awkde.GaussianKDE(glob_bw = self.bw, alpha = self.alpha, )
        else:
            # find marginal bws for multidim data
            bw = [kdepy.FFTKDE(bw = self.bw).bw(X[:,i:i+1]) for i in range(X.shape[-1])]
            self.estimator = kdepy.FFTKDE(kernel = self.kernel, bw = bw, )

        self.estimator.fit(X)
        return self

    def evaluate(self, data):
        data = self._check_X_2d(data)
        self._check_input_dims_match(data)
        if isinstance(self.estimator, awkde.GaussianKDE):
            return self.estimator.predict(data)
        else:
            evaluate = self.estimator.evaluate(self.kde_resolution)
            #kde_values = evaluate[0]
            #kde_pdf = evaluate[1]
            idxs = euclidean_distances(data,self._check_X_2d(evaluate[0])).argmin(axis = 1)
            return evaluate[1][idxs]

    def predict(self, X):
        return self.evaluate(X)

    def pdf(self, data):
        return self.evaluate(data)

    def rvs(self, size = 1, random_state = None):
        if isinstance(self.estimator, awkde.GaussianKDE):
            return self.estimator.sample(n_samples = size, random_state=random_state)
        else:
            values, probas = self.estimator.evaluate(self.kde_resolution)
            sampled_idxs = np.random.choice([*range(values.shape[0])], size = size)
            return values[sampled_idxs]

    def sample(self, size = 1, random_state = None):
        return self.rvs(size, random_state)

    def entropy(self, sample_size = 100):
        if isinstance(self.estimator, awkde.GaussianKDE):
            return np.mean(-np.log2(self.pdf(self.rvs(size = sample_size))))
        else:
            kde_pdf = self.estimator.evaluate(self.kde_resolution)[1]
            kde_pdf[kde_pdf<0] = 0 #assert non negativity
            kde_pdf = np.random.choice(kde_pdf,p = kde_pdf/kde_pdf.sum(), size = sample_size)
            return np.mean(-np.log2(kde_pdf))




# Cell

class RandomVariable():
    '''
    A container for distribution objects
    '''

    @classmethod
    def from_weights(cls, values, weights, n_samples = 100):
        data = np.random.choice(values,size = n_samples, p = weights)
        return cls.__init__(data,)

    def __init__(self, data, verbose = False):
        self.samples = data
        self.n_dim = 1 if len(data.shape) == 1 else data.shape[-1]
        self._fitted_dists = {}
        self.log_likelihood = []
        self.verbose = False
        return

    def __getitem__(self, item):
        if item == 'best':
            try:
                item = self._best_fit_alias
            except AttributeError:
                raise AttributeError('RandomVariable object has no "best" fit yet. Fit at least one density function through fit_dist method')

        return self._fitted_dists[item][0]

    #def __repr__(self):
    #    return f'RandomVariable({str(self.samples)})'

    def fit_best(self, candidates = ['norm','halfnorm','lognorm']):
        #fit all and make alias for best fit
        self._fit_all(self.samples, candidates)
        return self

    def _fit_ecdf(self, data, **ecdf_kwargs):
        self.ecdf = QuantileTransformer(**ecdf_kwargs).fit(data)
        return self

    def fit_dist(self, dist, **dist_kwargs):
        return self._fit_dist(self.samples,dist, **dist_kwargs)

    def _check_best(self):
        dists_aliases = list(self._fitted_dists)
        dists_arr = np.array([i[1] for i in self._fitted_dists.values()])
        best_fit_idx = np.argmax(dists_arr)
        self._best_fit_alias = dists_aliases[best_fit_idx]
        return

    def _fit_all(self, data, candidates):
        #TODO: check for multiplicity in candidates aliases
        for candidate in ctqdm(candidates, verbose = self.verbose):
            self.fit_dist(candidate)
        return self

    def _fit_dist(self, data, dist, **dist_kwargs):
        '''
        fits a specified distribution through scipy.stats.rv_continuous.fit method
        '''
        alias, dist_name = self._handle_dist_names(dist)
        alias, dist_class = self._get_dist_from_name(alias, dist_name)
        if alias.lower() == 'best':
            raise ValueError('"best" cannot be an alias for a distribution. its internally assgined to the best fit dist')
        if alias != 'kde':
            if self.n_dim > 1:
                raise ValueError('rv_continuous distributions is only available for 1d distributions. Use "kde" dist instead.')
            params = dist_class.fit(data)
            log_likelihood = np.log(np.product(dist_class.pdf(data,*params)))
            self._fitted_dists = {**self._fitted_dists, **{alias:(dist_class(*params),log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())
        else:
            dist = dist_class(**dist_kwargs).fit(data)
            log_likelihood = np.log(np.product(dist.pdf(data)))
            self._fitted_dists = {**self._fitted_dists, **{alias:(dist,log_likelihood)}}
            self.log_likelihood = list({**dict(self.log_likelihood), **{alias:log_likelihood}}.items())

        #update 'best' alias
        self._check_best()
        return self


    def _get_dist_from_name(self, alias, dist_name):
        '''
        handles dist_names. if str tries to get an attribute from scipy.stats accordingly
        that is also instance of scipy.stats.rv_continuous
        '''
        if isinstance(dist_name,str):
            if isinstance(getattr(stats,dist_name), stats.rv_continuous):
                alias = dist_name
                return (alias, getattr(stats,dist_name))
            elif dist_name.lower() == 'kde':
                alias = 'kde'
                return (alias, KDE)
            else:
                raise ValueError(f'dist must be a valid scipy.stats.rv_continuous instance, not {getattr(stats,dist_name)}')

        elif isinstance(dist_name, stats.rv_continuous):
            return (alias, dist_name)
        else:
            raise ValueError(f'dist must be a valid scipy.stats.rv_continuous instance or str, not {dist_name}')

    def _handle_dist_names(self, candidate_value):
        '''
        checks the inputs in elements of "candidates"
        returns a named tuple
        '''
        if isinstance(candidate_value, str):
            return candidate_value, candidate_value

        elif isinstance(candidate_value, tuple):

            if not len(candidate_value) == 2:
                raise ValueError(f'candidate named tuple must be of size 2, "{candidate_value}" has size {len(candidate_value)}')

            if not isinstance(candidate_value[0], str):
                raise ValueError(f'a candidate must be a str or named tuple (alias[str],<rv_continuous intance>), alias is of type {candidate_value[0].__class__}')

            else:
                return candidate_value

    def sample(self, size, dist = 'empirical', **kwargs):
        if dist == 'empirical':
            sampled_idxs = np.random.choice([*range(self.samples.shape[0])], size = size, **kwargs)
            return self.samples[sampled_idxs]
        else:
            return self[dist].rvs(size = size, **kwargs)

    def cdf(self, data, dist = 'empirical'):
        if dist == 'empirical':
            return self.ecdf.transform(data)
        else:
            return self[dist].cdf(data)

    def pdf(self, data, dist = 'best'):
        if dist == 'empirical':
            raise ValueError('empirical quantile distribution has no pdf definition')
        else:
            return self[dist].pdf(data)

    def ppf(self, data, dist = 'empirical'):
        if dist == 'empirical':
            return self.ecdf.inverse_transform(data)
        else:
            return self[dist].ppf(data)

    def entropy(self, dist):
        return self[dist].entropy()


# Cell
class RVArray:
    '''
    An array that contains RandomVariable objects and facilitates method calls and getting attributes
    '''
    def __init__(self, data):
        ''' the constructor recieves a list of RandomVariable items'''
        self.data = np.array(data)

    def __getattr__(self, attr):
        attr_list = [getattr(i,attr) for i in self.data]
        return RVArray(attr_list)

    def __call__(self, *args, **kwargs):
        results = [i(*args,**kwargs) for i in self.data]
        if all([isinstance(i,np.ndarray) for i in results]):
            return np.array(results)
        else:
            return RVArray(results)

    def __getitem__(self, *args):

        if len(args) > 1:
            return RVArray(self.data[args])
        else:
            if args[0].__class__ == str:
                return RVArray([i[args[0]] for i in self.data])
            else:
                return self.data[args]

    def __repr__(self):
        return f'RVArray({str(self.data)})'