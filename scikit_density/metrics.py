# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/04_metrics.ipynb (unless otherwise specified).

__all__ = ['quantile', 'quantile_value_sklearn', 'theoretical_entropy', 'kde_entropy', 'variance', 'covariance_matrix',
           'bimodal_variance', 'gaussian_distance_entropy', 'expected_distance_gaussian_likelihood',
           'distance_gaussian_likelihood', 'filter_borders']

# Cell
import numpy as np
from sklearn.metrics import pairwise
from scipy import stats

from .core.random_variable import KDE, RandomVariable
from .utils import (
    _fix_one_sample_2d, _fix_one_dist_2d, _fix_dist_1d,
    _fix_X_1d, _assert_dim_3d, _fix_one_dist_1d
)



# Cell

def quantile(y_true, pred_dist):
    '''checks in which quantile lies y_true, given the predicted distribution'''
    y_true = _fix_X_1d(y_true)
    y_true = _fix_one_sample_2d(y_true)
    pred_dist = _assert_dim_3d(pred_dist)
    assert y_true.shape[0] == pred_dist.shape[0], 'number of dists should be the same as number of points'
    return _fix_one_dist_2d(np.array([(y_true[i].T <= pred_dist[i].T).mean(axis = 1) for i in range(len(y_true))]))

def quantile_value_sklearn(y_true, pred_dist):
    '''checks in which quantile lies y_true, given the predicted distribution, using skleaerns QuantileTransformer'''
    return QuantileTransformer().fit(pred_dist).transform(y_true)

def theoretical_entropy(data, dist):
    return RandomVariable(data).fit_dist(dist).entropy(dist)

def kde_entropy(data, sample_size = 10000, bw = 'ISJ'):
    '''
    Calculates the entropy of multiple distributions
    input should be of shape (n_distributions, n_sample_per_distribution, n_dims_in_distribtuion)
    '''
    dist = _assert_dim_3d(data)
    return np.array([KDE(bw = bw).fit(d).entropy(sample_size = sample_size) for d in data])

def variance(data):
    '''
    Calculates the variance for each dimension (marginal) of multiple distributions
    input should be of shape (n_distributions, n_sample_per_distribution, n_dims_in_distribtuion)
    '''
    dist = _assert_dim_3d(data)
    return data.var(axis = -2)

def covariance_matrix(data):
    '''
    Calculates the variance for each dimension (marginal) of multiple distributions
    input should be of shape (n_distributions, n_sample_per_distribution, n_dims_in_distribtuion)
    '''


def bimodal_variance(data, pct_conv = 0.05, lb = 0.1,ub = 0.9):
    '''
    splits data in two according to the highest value of the derivative of cpdf
    and takes the wieghted average of the variance of the two ditributions generated
    '''
    #make split point
    #GENERALIZE FOR MULTIDIM
    data.sort()
    data = filter_borders(data,lb,ub)
    diff = np.diff(data)
    filter_size = np.floor(pct_conv*len(data))
    diff = np.convolve(diff,np.ones(filter_size)/filter_size, mode = 'same')
    split_point = data[np.argmax(diff)+1]
    #average variance
    arr1, arr2 = data[data >= split_point], data[data < split_point]
    var = len(arr1)*(arr1.var()) + len(arr2)*(arr2.var())


def gaussian_distance_entropy(data):
    '''
    calculates the entropy of the distribution of distances from centroid of points in dist, assuming normal distribution
    '''
    return -np.log(expected_distance_gaussian_likelihood(data))

def expected_distance_gaussian_likelihood(data):
    '''
    calculates the expected likelihood of the distances from centroid of samples in distributions in dist
    input should be of shape (n_distributions, n_sample_per_distribution, n_dims_in_distribtuion)
    '''
    dist = _assert_dim_3d(data)
    return np.array([distance_gaussian_likelihood(d).mean() for d in data])

def distance_gaussian_likelihood(data):
    '''
    calculates the expected likelihood of the distances from the centroid of samples in dist
    '''
    centroid = data.mean(axis = 0).reshape(1,-1)
    distances =  pairwise.euclidean_distances(data, centroid).flatten()
    distance_std = distances.std()
    if distance_std == 0:
        return 1
    z = (distances - distances.mean())/distance_std
    return 1/(distance_std*np.pi**(1/2))*np.exp(-1/2*z**2)

# Cell
def _make_outlier_filter(data, lb = 25, ub = 75, c = 1):
    a = np.array(data)
    upper_quartile = np.percentile(a, ub)
    lower_quartile = np.percentile(a, lb)
    iqr = (upper_quartile - lower_quartile) * c
    lb = lower_quartile - iqr
    ub = upper_quartile + iqr
    filter_ = np.zeros(a.shape)
    filter_[(a >= lb) & (a <= ub)] = 1
    return filter_

def filter_borders(data, lb = 0.05, ub = 0.95):
    lb = int(len(data)*lb)
    ub = int(len(data)*ub)
    return data[lb:ub]

#def distance_log_variance(dist):
#    '''variance of the distances of points to centroid of distribution'''
#    centroid = dist.mean(axis = 0).reshape(1,-1)
#    distances =  pairwise.euclidean_distances(dist, centroid).flatten()
#    return distances.var()